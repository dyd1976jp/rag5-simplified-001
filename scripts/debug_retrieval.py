#!/usr/bin/env python3
"""
æ£€ç´¢è°ƒè¯•è„šæœ¬

æä¾›å®Œæ•´çš„è°ƒè¯•æµç¨‹ï¼šæ£€æŸ¥æ•°æ®åº“ -> éªŒè¯åµŒå…¥ -> æµ‹è¯•æŸ¥è¯¢ -> ç”ŸæˆæŠ¥å‘Š
é’ˆå¯¹"äºæœ¦æœ§"æŸ¥è¯¢è¿›è¡Œä¸“é—¨æµ‹è¯•ï¼Œè¾“å‡ºè¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯å’Œå»ºè®®ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/debug_retrieval.py
    python scripts/debug_retrieval.py --query "äºæœ¦æœ§æ˜¯æ€ä¹ˆæ­»çš„"
    python scripts/debug_retrieval.py --collection knowledge_base --keyword "äºæœ¦æœ§"
"""

import argparse
import sys
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rag5.config import settings
from rag5.utils.logging_config import RAGLogger
from rag5.tools.diagnostics import QdrantInspector
from rag5.tools.embeddings import OllamaEmbeddingsManager
from rag5.tools.vectordb import QdrantManager
from rag5.tools.search import AdaptiveSearchTool, HybridSearchTool

# é…ç½®æ—¥å¿—
RAGLogger.setup_logging(
    log_level="DEBUG",
    log_file=None,  # ä»…è¾“å‡ºåˆ°æ§åˆ¶å°
    enable_console=True
)
logger = logging.getLogger(__name__)


class RetrievalDebugger:
    """
    æ£€ç´¢è°ƒè¯•å™¨
    
    æä¾›å®Œæ•´çš„è°ƒè¯•æµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®åº“æ£€æŸ¥ã€åµŒå…¥éªŒè¯ã€æŸ¥è¯¢æµ‹è¯•å’Œé—®é¢˜è¯Šæ–­ã€‚
    """
    
    def __init__(
        self,
        collection_name: Optional[str] = None,
        qdrant_url: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–è°ƒè¯•å™¨
        
        å‚æ•°:
            collection_name: é›†åˆåç§°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰
            qdrant_url: Qdrant URLï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰
        """
        self.collection_name = collection_name or settings.collection_name
        self.qdrant_url = qdrant_url or settings.qdrant_url
        
        # åˆå§‹åŒ–ç®¡ç†å™¨
        self.qdrant_manager = QdrantManager(url=self.qdrant_url)
        self.embeddings_manager = OllamaEmbeddingsManager(
            model=settings.embed_model,
            base_url=settings.ollama_host
        )
        self.embeddings_manager.initialize()
        
        # åˆå§‹åŒ–æ£€æŸ¥å™¨
        self.inspector = QdrantInspector(
            self.qdrant_manager,
            self.embeddings_manager
        )
        
        # åˆå§‹åŒ–æœç´¢å·¥å…·
        self.adaptive_search = AdaptiveSearchTool(
            self.embeddings_manager,
            self.qdrant_manager,
            self.collection_name
        )
        
        self.hybrid_search = HybridSearchTool(
            self.embeddings_manager,
            self.qdrant_manager,
            self.collection_name
        )
        
        # è¯Šæ–­ç»“æœ
        self.issues: List[Dict[str, Any]] = []
        self.recommendations: List[str] = []
    
    def print_header(self, title: str):
        """æ‰“å°æ ‡é¢˜"""
        print("\n" + "=" * 80)
        print(f"  {title}")
        print("=" * 80 + "\n")
    
    def print_section(self, title: str):
        """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
        print(f"\n{'â”€' * 80}")
        print(f"  {title}")
        print(f"{'â”€' * 80}\n")
    
    def check_database_status(self) -> Dict[str, Any]:
        """
        æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
        
        è¿”å›:
            æ•°æ®åº“çŠ¶æ€ä¿¡æ¯
        """
        self.print_section("1. æ•°æ®åº“çŠ¶æ€æ£€æŸ¥")
        
        # è·å–é›†åˆç»Ÿè®¡
        stats = self.inspector.get_collection_stats(self.collection_name)
        
        print(f"é›†åˆåç§°: {self.collection_name}")
        print(f"Qdrant URL: {self.qdrant_url}")
        print()
        
        if not stats['exists']:
            print("âŒ é›†åˆä¸å­˜åœ¨")
            self.issues.append({
                "type": "database_empty",
                "severity": "critical",
                "message": f"é›†åˆ '{self.collection_name}' ä¸å­˜åœ¨"
            })
            self.recommendations.append(
                f"è¿è¡Œç´¢å¼•å‘½ä»¤åˆ›å»ºé›†åˆå¹¶ç´¢å¼•æ–‡æ¡£:\n"
                f"  python -m rag5.tools.index_manager reindex --directory ./docs --collection {self.collection_name}"
            )
            return stats
        
        print(f"âœ“ é›†åˆå­˜åœ¨")
        print(f"  - çŠ¶æ€: {stats['status']}")
        print(f"  - ç‚¹æ•°é‡: {stats['points_count']}")
        print(f"  - å‘é‡æ•°é‡: {stats['vectors_count']}")
        print(f"  - å·²ç´¢å¼•å‘é‡: {stats['indexed_vectors_count']}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
        if stats['points_count'] == 0:
            print("\nâŒ æ•°æ®åº“ä¸ºç©ºï¼Œæ²¡æœ‰ç´¢å¼•ä»»ä½•æ–‡æ¡£")
            self.issues.append({
                "type": "database_empty",
                "severity": "critical",
                "message": "æ•°æ®åº“ä¸­æ²¡æœ‰ä»»ä½•æ–‡æ¡£"
            })
            self.recommendations.append(
                "ç´¢å¼•æ–‡æ¡£åˆ°æ•°æ®åº“:\n"
                f"  python scripts/ingest.py --directory ./docs"
            )
        else:
            print(f"\nâœ“ æ•°æ®åº“åŒ…å« {stats['points_count']} ä¸ªæ–‡æ¡£åˆ†å—")
        
        # è·å–æ ·æœ¬æ•°æ®
        print("\nè·å–æ ·æœ¬æ•°æ®...")
        samples = self.inspector.get_sample_points(self.collection_name, limit=3)
        
        if samples:
            print(f"\næ ·æœ¬æ•°æ® (å‰ {len(samples)} ä¸ª):")
            for i, sample in enumerate(samples, 1):
                print(f"\n  æ ·æœ¬ {i}:")
                print(f"    - ID: {sample['id']}")
                print(f"    - å‘é‡ç»´åº¦: {sample['vector_dim']}")
                print(f"    - åŒ…å«æ–‡æœ¬: {'æ˜¯' if sample['has_text'] else 'å¦'}")
                print(f"    - åŒ…å«æ¥æº: {'æ˜¯' if sample['has_source'] else 'å¦'}")
                if sample['text_preview']:
                    preview = sample['text_preview'].replace('\n', ' ')
                    print(f"    - æ–‡æœ¬é¢„è§ˆ: {preview}...")
        
        return stats
    
    def search_keyword(self, keyword: str) -> List[Dict[str, Any]]:
        """
        æœç´¢å…³é”®è¯
        
        å‚æ•°:
            keyword: æœç´¢å…³é”®è¯
        
        è¿”å›:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        self.print_section(f"2. å…³é”®è¯æœç´¢æµ‹è¯•: '{keyword}'")
        
        print(f"åœ¨æ•°æ®åº“ä¸­æœç´¢å…³é”®è¯ '{keyword}'...")
        results = self.inspector.search_by_keyword(
            self.collection_name,
            keyword,
            limit=10
        )
        
        if not results:
            print(f"\nâŒ æœªæ‰¾åˆ°åŒ…å« '{keyword}' çš„æ–‡æ¡£")
            self.issues.append({
                "type": "keyword_not_found",
                "severity": "high",
                "message": f"æ•°æ®åº“ä¸­ä¸åŒ…å«å…³é”®è¯ '{keyword}'"
            })
            self.recommendations.append(
                f"ç¡®è®¤æ–‡æ¡£ä¸­æ˜¯å¦åŒ…å« '{keyword}'ï¼Œå¦‚æœåŒ…å«ï¼Œè¯·é‡æ–°ç´¢å¼•:\n"
                f"  python -m rag5.tools.index_manager reindex --directory ./docs --force"
            )
        else:
            print(f"\nâœ“ æ‰¾åˆ° {len(results)} ä¸ªåŒ…å« '{keyword}' çš„æ–‡æ¡£åˆ†å—")
            for i, result in enumerate(results[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"\n  ç»“æœ {i}:")
                print(f"    - æ¥æº: {result['source']}")
                print(f"    - å‡ºç°æ¬¡æ•°: {result['keyword_count']}")
                print(f"    - æ–‡æœ¬ç‰‡æ®µ: {result['text']}")
        
        return results
    
    def verify_embeddings(self) -> Dict[str, Any]:
        """
        éªŒè¯åµŒå…¥æ¨¡å‹
        
        è¿”å›:
            éªŒè¯ç»“æœ
        """
        self.print_section("3. åµŒå…¥æ¨¡å‹éªŒè¯")
        
        print(f"éªŒè¯åµŒå…¥æ¨¡å‹: {settings.embed_model}")
        print(f"Ollama åœ°å€: {settings.ollama_host}")
        print()
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬",
            "äººå·¥æ™ºèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ",
            "äºæœ¦æœ§æ˜¯è°ï¼Ÿ"
        ]
        
        result = self.inspector.verify_embeddings(
            self.collection_name,
            test_texts
        )
        
        if result['model_working']:
            print(f"âœ“ åµŒå…¥æ¨¡å‹å·¥ä½œæ­£å¸¸")
            print(f"  - æ¨¡å‹åç§°: {result['model_name']}")
            print(f"  - å‘é‡ç»´åº¦: {result['vector_dim']}")
            print(f"  - æœŸæœ›ç»´åº¦: {result['expected_dim']}")
            print(f"  - ç»´åº¦åŒ¹é…: {'æ˜¯' if result['dimension_match'] else 'å¦'}")
            print(f"  - å¹³å‡ç”Ÿæˆæ—¶é—´: {result['average_time']:.3f}ç§’")
            print(f"  - æˆåŠŸæµ‹è¯•: {result['successful_tests']}/{result['total_tests']}")
        else:
            print(f"âŒ åµŒå…¥æ¨¡å‹éªŒè¯å¤±è´¥")
            print(f"  - é”™è¯¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            self.issues.append({
                "type": "embedding_model_error",
                "severity": "critical",
                "message": f"åµŒå…¥æ¨¡å‹éªŒè¯å¤±è´¥: {result.get('error')}"
            })
            self.recommendations.append(
                "æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ:\n"
                "  curl http://localhost:11434/api/tags\n"
                f"ç¡®è®¤æ¨¡å‹ '{settings.embed_model}' å·²å®‰è£…:\n"
                f"  ollama pull {settings.embed_model}"
            )
        
        # æ˜¾ç¤ºæµ‹è¯•è¯¦æƒ…
        if result.get('test_results'):
            print("\næµ‹è¯•è¯¦æƒ…:")
            for i, test in enumerate(result['test_results'], 1):
                status = "âœ“" if test['success'] else "âœ—"
                print(f"  {status} æµ‹è¯• {i}: {test['text']}")
                if test['success']:
                    print(f"      ç»´åº¦: {test['vector_dim']}, è€—æ—¶: {test['time']:.3f}ç§’")
                else:
                    print(f"      é”™è¯¯: {test['error']}")
        
        return result
    
    def test_query(
        self,
        query: str,
        test_thresholds: Optional[List[float]] = None
    ) -> Dict[str, Any]:
        """
        æµ‹è¯•æŸ¥è¯¢
        
        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            test_thresholds: è¦æµ‹è¯•çš„é˜ˆå€¼åˆ—è¡¨
        
        è¿”å›:
            æµ‹è¯•ç»“æœ
        """
        self.print_section(f"4. æŸ¥è¯¢æµ‹è¯•: '{query}'")
        
        if test_thresholds is None:
            test_thresholds = [0.7, 0.5, 0.3, 0.1]
        
        print(f"æŸ¥è¯¢: {query}")
        print(f"æµ‹è¯•é˜ˆå€¼: {test_thresholds}")
        print()
        
        # æµ‹è¯•ä¸åŒé˜ˆå€¼
        print("æµ‹è¯•ä¸åŒç›¸ä¼¼åº¦é˜ˆå€¼...")
        stats = self.adaptive_search.get_search_statistics(query, test_thresholds)
        
        if 'error' in stats:
            print(f"âŒ æŸ¥è¯¢æµ‹è¯•å¤±è´¥: {stats['error']}")
            return stats
        
        print("\né˜ˆå€¼æµ‹è¯•ç»“æœ:")
        print(f"{'é˜ˆå€¼':<10} {'ç»“æœæ•°':<10} {'å¹³å‡åˆ†æ•°':<15}")
        print("-" * 40)
        
        for threshold in sorted(test_thresholds, reverse=True):
            result_info = stats['results_by_threshold'].get(threshold, {})
            count = result_info.get('count', 0)
            avg_score = result_info.get('avg_score', 0)
            print(f"{threshold:<10.2f} {count:<10} {avg_score:<15.4f}")
        
        # ä½¿ç”¨è‡ªé€‚åº”æœç´¢
        print(f"\nä½¿ç”¨è‡ªé€‚åº”æœç´¢ (åˆå§‹é˜ˆå€¼: {settings.similarity_threshold})...")
        adaptive_results = self.adaptive_search.search_with_fallback(
            query=query,
            initial_threshold=settings.similarity_threshold,
            min_threshold=settings.min_similarity_threshold,
            target_results=settings.target_results
        )
        
        if not adaptive_results:
            print(f"\nâŒ è‡ªé€‚åº”æœç´¢æœªæ‰¾åˆ°ç»“æœ")
            self.issues.append({
                "type": "no_results",
                "severity": "high",
                "message": f"æŸ¥è¯¢ '{query}' æœªè¿”å›ä»»ä½•ç»“æœ"
            })
            self.recommendations.append(
                "å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆ:\n"
                "  1. ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡é«˜ - å°è¯•é™ä½ SIMILARITY_THRESHOLD\n"
                "  2. æ–‡æ¡£æœªæ­£ç¡®ç´¢å¼• - é‡æ–°ç´¢å¼•æ–‡æ¡£\n"
                "  3. æŸ¥è¯¢è¯ä¸åŒ¹é… - å°è¯•ä½¿ç”¨ä¸åŒçš„æŸ¥è¯¢è¯\n"
                "  4. åµŒå…¥æ¨¡å‹ä¸å…¼å®¹ - æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒä¸­æ–‡"
            )
        else:
            print(f"\nâœ“ æ‰¾åˆ° {len(adaptive_results)} ä¸ªç»“æœ")
            for i, result in enumerate(adaptive_results[:3], 1):  # æ˜¾ç¤ºå‰3ä¸ª
                print(f"\n  ç»“æœ {i}:")
                print(f"    - ç›¸ä¼¼åº¦åˆ†æ•°: {result['score']:.4f}")
                print(f"    - æ¥æº: {result['source']}")
                content_preview = result['content'][:200].replace('\n', ' ')
                print(f"    - å†…å®¹: {content_preview}...")
        
        # å¦‚æœå¯ç”¨æ··åˆæœç´¢ï¼Œä¹Ÿæµ‹è¯•æ··åˆæœç´¢
        if settings.enable_hybrid_search:
            print(f"\nä½¿ç”¨æ··åˆæœç´¢...")
            hybrid_results = self.hybrid_search.hybrid_search(
                query=query,
                vector_weight=settings.vector_search_weight,
                keyword_weight=settings.keyword_search_weight
            )
            
            if hybrid_results:
                print(f"âœ“ æ··åˆæœç´¢æ‰¾åˆ° {len(hybrid_results)} ä¸ªç»“æœ")
                for i, result in enumerate(hybrid_results[:3], 1):
                    print(f"\n  ç»“æœ {i}:")
                    print(f"    - ç»¼åˆåˆ†æ•°: {result['score']:.4f}")
                    print(f"    - æ¥æº: {result['source']}")
                    content_preview = result['content'][:200].replace('\n', ' ')
                    print(f"    - å†…å®¹: {content_preview}...")
        
        return {
            "statistics": stats,
            "adaptive_results": adaptive_results,
            "hybrid_results": hybrid_results if settings.enable_hybrid_search else None
        }
    
    def diagnose_issues(self):
        """è¯Šæ–­é—®é¢˜å¹¶æä¾›å»ºè®®"""
        self.print_section("5. é—®é¢˜è¯Šæ–­å’Œå»ºè®®")
        
        if not self.issues:
            print("âœ“ æœªå‘ç°æ˜æ˜¾é—®é¢˜")
            print("\nç³»ç»ŸçŠ¶æ€è‰¯å¥½ï¼Œæ£€ç´¢åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
            return
        
        print(f"å‘ç° {len(self.issues)} ä¸ªé—®é¢˜:\n")
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        critical_issues = [i for i in self.issues if i['severity'] == 'critical']
        high_issues = [i for i in self.issues if i['severity'] == 'high']
        medium_issues = [i for i in self.issues if i['severity'] == 'medium']
        
        if critical_issues:
            print("ğŸ”´ ä¸¥é‡é—®é¢˜:")
            for i, issue in enumerate(critical_issues, 1):
                print(f"  {i}. [{issue['type']}] {issue['message']}")
            print()
        
        if high_issues:
            print("ğŸŸ  é«˜ä¼˜å…ˆçº§é—®é¢˜:")
            for i, issue in enumerate(high_issues, 1):
                print(f"  {i}. [{issue['type']}] {issue['message']}")
            print()
        
        if medium_issues:
            print("ğŸŸ¡ ä¸­ç­‰ä¼˜å…ˆçº§é—®é¢˜:")
            for i, issue in enumerate(medium_issues, 1):
                print(f"  {i}. [{issue['type']}] {issue['message']}")
            print()
        
        # æä¾›å»ºè®®
        if self.recommendations:
            print("\nå»ºè®®çš„è§£å†³æ–¹æ¡ˆ:\n")
            for i, rec in enumerate(self.recommendations, 1):
                print(f"{i}. {rec}")
                print()
    
    def generate_report(self) -> str:
        """
        ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        
        è¿”å›:
            æŠ¥å‘Šæ–‡æœ¬
        """
        self.print_section("6. è¯Šæ–­æŠ¥å‘Š")
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
æ£€ç´¢ç³»ç»Ÿè¯Šæ–­æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {timestamp}
é›†åˆåç§°: {self.collection_name}
Qdrant URL: {self.qdrant_url}

é…ç½®ä¿¡æ¯:
  - åµŒå…¥æ¨¡å‹: {settings.embed_model}
  - ç›¸ä¼¼åº¦é˜ˆå€¼: {settings.similarity_threshold}
  - Top K: {settings.top_k}
  - æ··åˆæœç´¢: {'å¯ç”¨' if settings.enable_hybrid_search else 'ç¦ç”¨'}
  - ä¸­æ–‡åˆ†å—å™¨: {'å¯ç”¨' if settings.enable_chinese_splitter else 'ç¦ç”¨'}

é—®é¢˜æ€»æ•°: {len(self.issues)}
  - ä¸¥é‡: {len([i for i in self.issues if i['severity'] == 'critical'])}
  - é«˜ä¼˜å…ˆçº§: {len([i for i in self.issues if i['severity'] == 'high'])}
  - ä¸­ç­‰ä¼˜å…ˆçº§: {len([i for i in self.issues if i['severity'] == 'medium'])}

å»ºè®®æ•°é‡: {len(self.recommendations)}
"""
        
        print(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_dir = project_root / "logs"
        report_dir.mkdir(exist_ok=True)
        report_file = report_dir / f"debug_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
            f.write("\nè¯¦ç»†é—®é¢˜åˆ—è¡¨:\n")
            for i, issue in enumerate(self.issues, 1):
                f.write(f"\n{i}. [{issue['severity'].upper()}] {issue['type']}\n")
                f.write(f"   {issue['message']}\n")
            
            f.write("\nå»ºè®®çš„è§£å†³æ–¹æ¡ˆ:\n")
            for i, rec in enumerate(self.recommendations, 1):
                f.write(f"\n{i}. {rec}\n")
        
        print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return report
    
    def run_full_diagnostic(
        self,
        test_query: str = "äºæœ¦æœ§æ˜¯æ€ä¹ˆæ­»çš„",
        test_keyword: str = "äºæœ¦æœ§"
    ):
        """
        è¿è¡Œå®Œæ•´çš„è¯Šæ–­æµç¨‹
        
        å‚æ•°:
            test_query: æµ‹è¯•æŸ¥è¯¢
            test_keyword: æµ‹è¯•å…³é”®è¯
        """
        self.print_header("RAG5 æ£€ç´¢ç³»ç»Ÿè¯Šæ–­å·¥å…·")
        
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"æµ‹è¯•æŸ¥è¯¢: {test_query}")
        print(f"æµ‹è¯•å…³é”®è¯: {test_keyword}")
        
        try:
            # 1. æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
            db_stats = self.check_database_status()
            
            # å¦‚æœæ•°æ®åº“ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œè·³è¿‡åç»­æµ‹è¯•
            if not db_stats.get('exists') or db_stats.get('points_count', 0) == 0:
                self.diagnose_issues()
                self.generate_report()
                return
            
            # 2. æœç´¢å…³é”®è¯
            keyword_results = self.search_keyword(test_keyword)
            
            # 3. éªŒè¯åµŒå…¥æ¨¡å‹
            embedding_result = self.verify_embeddings()
            
            # å¦‚æœåµŒå…¥æ¨¡å‹æœ‰é—®é¢˜ï¼Œè·³è¿‡æŸ¥è¯¢æµ‹è¯•
            if not embedding_result.get('model_working'):
                self.diagnose_issues()
                self.generate_report()
                return
            
            # 4. æµ‹è¯•æŸ¥è¯¢
            query_results = self.test_query(test_query)
            
            # 5. è¯Šæ–­é—®é¢˜
            self.diagnose_issues()
            
            # 6. ç”ŸæˆæŠ¥å‘Š
            self.generate_report()
            
        except Exception as e:
            logger.error(f"è¯Šæ–­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            print(f"\nâŒ è¯Šæ–­å¤±è´¥: {e}")
            self.issues.append({
                "type": "diagnostic_error",
                "severity": "critical",
                "message": f"è¯Šæ–­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            })
            self.generate_report()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="RAG5 æ£€ç´¢ç³»ç»Ÿè°ƒè¯•å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¿è¡Œå®Œæ•´è¯Šæ–­ï¼ˆä½¿ç”¨é»˜è®¤æŸ¥è¯¢ï¼‰
  python scripts/debug_retrieval.py
  
  # ä½¿ç”¨è‡ªå®šä¹‰æŸ¥è¯¢
  python scripts/debug_retrieval.py --query "æå°å‹‡æ˜¯è°"
  
  # ä½¿ç”¨è‡ªå®šä¹‰å…³é”®è¯
  python scripts/debug_retrieval.py --keyword "æå°å‹‡"
  
  # æŒ‡å®šé›†åˆåç§°
  python scripts/debug_retrieval.py --collection my_collection
        """
    )
    
    parser.add_argument(
        '--query',
        type=str,
        default="äºæœ¦æœ§æ˜¯æ€ä¹ˆæ­»çš„",
        help="æµ‹è¯•æŸ¥è¯¢ï¼ˆé»˜è®¤: äºæœ¦æœ§æ˜¯æ€ä¹ˆæ­»çš„ï¼‰"
    )
    
    parser.add_argument(
        '--keyword',
        type=str,
        default="äºæœ¦æœ§",
        help="æµ‹è¯•å…³é”®è¯ï¼ˆé»˜è®¤: äºæœ¦æœ§ï¼‰"
    )
    
    parser.add_argument(
        '--collection',
        type=str,
        default=None,
        help=f"é›†åˆåç§°ï¼ˆé»˜è®¤: {settings.collection_name}ï¼‰"
    )
    
    parser.add_argument(
        '--qdrant-url',
        type=str,
        default=None,
        help=f"Qdrant URLï¼ˆé»˜è®¤: {settings.qdrant_url}ï¼‰"
    )
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºè°ƒè¯•å™¨
        debugger = RetrievalDebugger(
            collection_name=args.collection,
            qdrant_url=args.qdrant_url
        )
        
        # è¿è¡Œå®Œæ•´è¯Šæ–­
        debugger.run_full_diagnostic(
            test_query=args.query,
            test_keyword=args.keyword
        )
        
    except KeyboardInterrupt:
        print("\n\nç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
