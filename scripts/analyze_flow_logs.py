#!/usr/bin/env python3
"""
Command-line interface for analyzing unified flow logs.

This script provides utilities for filtering, analyzing, and exporting
unified flow log files generated by the RAG5 system.

Usage:
    # Show timing statistics
    python analyze_flow_logs.py --log-file logs/unified_flow.log --stats
    
    # Filter by session
    python analyze_flow_logs.py --log-file logs/unified_flow.log --session abc-123-def
    
    # Find errors
    python analyze_flow_logs.py --log-file logs/unified_flow.log --errors
    
    # Find slow operations
    python analyze_flow_logs.py --log-file logs/unified_flow.log --slow 5.0
    
    # Export to JSON
    python analyze_flow_logs.py --log-file logs/unified_flow.log --export-json output.json
    
    # Export to CSV
    python analyze_flow_logs.py --log-file logs/unified_flow.log --export-csv output.csv
    
    # Combine multiple operations
    python analyze_flow_logs.py --log-file logs/unified_flow.log --stats --errors --slow 3.0
"""

import argparse
import sys
from pathlib import Path
from typing import Optional

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from rag5.utils.flow_analyzer import FlowLogAnalyzer


def format_duration(seconds: float) -> str:
    """
    Format duration in a human-readable way.
    
    Args:
        seconds: Duration in seconds
        
    Returns:
        Formatted duration string
    """
    if seconds < 1.0:
        return f"{seconds*1000:.0f}ms"
    elif seconds < 60.0:
        return f"{seconds:.2f}s"
    else:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}m {secs:.1f}s"


def print_header(title: str, width: int = 80) -> None:
    """Print a formatted header."""
    print("\n" + "=" * width)
    print(title.center(width))
    print("=" * width)


def print_section(title: str, width: int = 80) -> None:
    """Print a formatted section header."""
    print("\n" + "-" * width)
    print(title)
    print("-" * width)


def handle_session_filter(analyzer: FlowLogAnalyzer, session_id: str) -> None:
    """
    Handle session filtering command.
    
    Args:
        analyzer: FlowLogAnalyzer instance
        session_id: Session ID to filter by
    """
    print_header(f"Session Filter: {session_id}")
    
    entries = analyzer.filter_by_session(session_id)
    
    if not entries:
        print(f"\nNo entries found for session: {session_id}")
        return
    
    print(f"\nFound {len(entries)} entries for session {session_id}")
    print("\nTimeline:")
    print(f"{'Time':>10}  {'Event Type':30}  {'Details':40}")
    print("-" * 85)
    
    for entry in entries:
        elapsed = entry.get('elapsed_time', 0)
        event_type = entry['event_type']
        
        # Extract relevant details
        metadata = entry.get('metadata', {})
        details = ""
        
        if event_type in ('TOOL_EXECUTION', 'TOOL_EXEC'):
            tool_name = metadata.get('tool_name', 'unknown')
            duration = metadata.get('duration', 0)
            status = metadata.get('status', 'unknown')
            details = f"{tool_name} [{format_duration(duration)}] {status}"
        
        elif event_type == 'LLM_CALL':
            model = metadata.get('model', 'unknown')
            duration = metadata.get('duration', 0)
            details = f"{model} [{format_duration(duration)}]"
        
        elif event_type == 'ERROR':
            error_type = metadata.get('error_type', 'unknown')
            details = f"{error_type}"
        
        elif event_type in ('QUERY_COMPLETE', 'COMPLETE'):
            status = metadata.get('status', 'unknown')
            total_duration = metadata.get('total_duration', elapsed)
            details = f"{status} [total: {format_duration(total_duration)}]"
        
        print(f"{elapsed:>9.3f}s  {event_type:30}  {details:40}")


def handle_statistics(analyzer: FlowLogAnalyzer) -> None:
    """
    Handle statistics display command.
    
    Args:
        analyzer: FlowLogAnalyzer instance
    """
    print_header("Timing Statistics")
    
    stats = analyzer.get_timing_stats()
    
    if not stats:
        print("\nNo timing statistics available.")
        return
    
    for op_type, op_stats in stats.items():
        print_section(op_type.replace('_', ' ').title())
        
        print(f"  Count:       {op_stats['count']}")
        print(f"  Average:     {format_duration(op_stats['avg'])}")
        print(f"  Minimum:     {format_duration(op_stats['min'])}")
        print(f"  Maximum:     {format_duration(op_stats['max'])}")
        print(f"  P95:         {format_duration(op_stats['p95'])}")


def handle_errors(analyzer: FlowLogAnalyzer) -> None:
    """
    Handle error display command.
    
    Args:
        analyzer: FlowLogAnalyzer instance
    """
    print_header("Error Report")
    
    errors = analyzer.find_errors()
    
    if not errors:
        print("\nNo errors found. ✓")
        return
    
    print(f"\nFound {len(errors)} error(s):")
    
    for i, error in enumerate(errors, 1):
        print_section(f"Error {i}")
        
        print(f"  Session:     {error.get('session_id', 'unknown')}")
        print(f"  Timestamp:   {error.get('timestamp', 'unknown')}")
        print(f"  Elapsed:     {format_duration(error.get('elapsed_time', 0))}")
        print(f"  Type:        {error.get('error_type', 'unknown')}")
        print(f"  Message:     {error.get('error_message', 'unknown')}")


def handle_slow_operations(analyzer: FlowLogAnalyzer, threshold: float) -> None:
    """
    Handle slow operations display command.
    
    Args:
        analyzer: FlowLogAnalyzer instance
        threshold: Time threshold in seconds
    """
    print_header(f"Slow Operations (>{format_duration(threshold)})")
    
    slow_ops = analyzer.find_slow_operations(threshold_seconds=threshold)
    
    if not slow_ops:
        print(f"\nNo operations found exceeding {format_duration(threshold)}. ✓")
        return
    
    print(f"\nFound {len(slow_ops)} slow operation(s):")
    print(f"\n{'Operation':40}  {'Duration':>12}  {'Session':30}")
    print("-" * 85)
    
    for op in slow_ops:
        operation = op['operation']
        duration = op['duration']
        session_id = op.get('session_id', 'unknown')
        
        print(f"{operation:40}  {format_duration(duration):>12}  {session_id:30}")


def handle_export_json(analyzer: FlowLogAnalyzer, output_file: str) -> None:
    """
    Handle JSON export command.
    
    Args:
        analyzer: FlowLogAnalyzer instance
        output_file: Output file path
    """
    print_header("Export to JSON")
    
    try:
        analyzer.export_to_json(output_file)
        
        # Get file size
        file_size = Path(output_file).stat().st_size
        
        print(f"\n✓ Successfully exported to: {output_file}")
        print(f"  Entries:     {len(analyzer.entries)}")
        print(f"  File size:   {file_size:,} bytes")
        
    except Exception as e:
        print(f"\n✗ Failed to export to JSON: {e}")
        sys.exit(1)


def handle_export_csv(analyzer: FlowLogAnalyzer, output_file: str) -> None:
    """
    Handle CSV export command.
    
    Args:
        analyzer: FlowLogAnalyzer instance
        output_file: Output file path
    """
    print_header("Export to CSV")
    
    try:
        analyzer.export_to_csv(output_file)
        
        # Get file size
        file_size = Path(output_file).stat().st_size
        
        print(f"\n✓ Successfully exported to: {output_file}")
        print(f"  Entries:     {len(analyzer.entries)}")
        print(f"  File size:   {file_size:,} bytes")
        
    except Exception as e:
        print(f"\n✗ Failed to export to CSV: {e}")
        sys.exit(1)


def main():
    """Main entry point for the CLI."""
    parser = argparse.ArgumentParser(
        description="Analyze unified flow logs from RAG5 system",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Show timing statistics
  %(prog)s --log-file logs/unified_flow.log --stats
  
  # Filter by session
  %(prog)s --log-file logs/unified_flow.log --session abc-123-def
  
  # Find errors
  %(prog)s --log-file logs/unified_flow.log --errors
  
  # Find slow operations (>5 seconds)
  %(prog)s --log-file logs/unified_flow.log --slow 5.0
  
  # Export to JSON
  %(prog)s --log-file logs/unified_flow.log --export-json output.json
  
  # Export to CSV
  %(prog)s --log-file logs/unified_flow.log --export-csv output.csv
  
  # Combine multiple operations
  %(prog)s --log-file logs/unified_flow.log --stats --errors --slow 3.0
        """
    )
    
    # Required arguments
    parser.add_argument(
        '--log-file',
        type=str,
        required=True,
        help='Path to unified flow log file'
    )
    
    # Analysis commands
    parser.add_argument(
        '--session',
        type=str,
        metavar='SESSION_ID',
        help='Filter log entries by session ID'
    )
    
    parser.add_argument(
        '--stats',
        action='store_true',
        help='Show timing statistics'
    )
    
    parser.add_argument(
        '--errors',
        action='store_true',
        help='Show all errors'
    )
    
    parser.add_argument(
        '--slow',
        type=float,
        metavar='THRESHOLD',
        help='Find slow operations exceeding THRESHOLD seconds (e.g., 5.0)'
    )
    
    # Export commands
    parser.add_argument(
        '--export-json',
        type=str,
        metavar='FILE',
        help='Export log entries to JSON file'
    )
    
    parser.add_argument(
        '--export-csv',
        type=str,
        metavar='FILE',
        help='Export log entries to CSV file'
    )
    
    args = parser.parse_args()
    
    # Check if log file exists
    log_path = Path(args.log_file)
    if not log_path.exists():
        print(f"Error: Log file not found: {args.log_file}", file=sys.stderr)
        sys.exit(1)
    
    # Check if at least one command is specified
    if not any([
        args.session,
        args.stats,
        args.errors,
        args.slow is not None,
        args.export_json,
        args.export_csv
    ]):
        parser.print_help()
        print("\nError: At least one command must be specified", file=sys.stderr)
        sys.exit(1)
    
    # Initialize analyzer
    try:
        analyzer = FlowLogAnalyzer(args.log_file)
    except Exception as e:
        print(f"Error: Failed to load log file: {e}", file=sys.stderr)
        sys.exit(1)
    
    # Print summary
    print(f"Loaded log file: {args.log_file}")
    print(f"Total entries: {len(analyzer.entries)}")
    
    # Execute commands in order
    if args.session:
        handle_session_filter(analyzer, args.session)
    
    if args.stats:
        handle_statistics(analyzer)
    
    if args.errors:
        handle_errors(analyzer)
    
    if args.slow is not None:
        handle_slow_operations(analyzer, args.slow)
    
    if args.export_json:
        handle_export_json(analyzer, args.export_json)
    
    if args.export_csv:
        handle_export_csv(analyzer, args.export_csv)
    
    print("\n" + "=" * 80)
    print("Analysis complete")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
